<hr>
<h2 id="title-面向端侧场景的低时延大模型推理优化技术–大创日记（1）date-2025-05-23-15-22-53"><a href="#title-面向端侧场景的低时延大模型推理优化技术–大创日记（1）date-2025-05-23-15-22-53" class="headerlink" title="title: 面向端侧场景的低时延大模型推理优化技术–大创日记（1）date: 2025-05-23 15:22:53"></a>title: 面向端侧场景的低时延大模型推理优化技术–大创日记（1）<br>date: 2025-05-23 15:22:53</h2><h1 id="2025-4-3"><a href="#2025-4-3" class="headerlink" title="2025&#x2F;4&#x2F;3"></a>2025&#x2F;4&#x2F;3</h1><h3 id="在跑readme里的实例代码时出现了第一个环境问题"><a href="#在跑readme里的实例代码时出现了第一个环境问题" class="headerlink" title="在跑readme里的实例代码时出现了第一个环境问题"></a>在跑readme里的实例代码时出现了第一个环境问题</h3><blockquote>
<p>cannot import name ‘oneshot’ from ‘llmcompressor’</p>
</blockquote>
<h3 id="我们可以用下面的代码检查库里的函数"><a href="#我们可以用下面的代码检查库里的函数" class="headerlink" title="我们可以用下面的代码检查库里的函数"></a>我们可以用下面的代码检查库里的函数</h3><pre><code class="python">import llmcompressor
print(dir(llmcompressor))
</code></pre>
<h3 id="发现确实是没有对应的函数"><a href="#发现确实是没有对应的函数" class="headerlink" title="发现确实是没有对应的函数"></a>发现确实是没有对应的函数</h3><h3 id="我们可以查阅这段评论里尝试找到解决方法"><a href="#我们可以查阅这段评论里尝试找到解决方法" class="headerlink" title="我们可以查阅这段评论里尝试找到解决方法"></a>我们可以查阅这段评论里尝试找到解决方法</h3><blockquote>
<p><a href="https://github.com/vllm-project/llm-compressor/issues/1257">https://github.com/vllm-project/llm-compressor/issues/1257</a></p>
</blockquote>
<h1 id="2025-4-12"><a href="#2025-4-12" class="headerlink" title="2025&#x2F;4&#x2F;12"></a>2025&#x2F;4&#x2F;12</h1><h3 id="解决师兄指定模型跑时的环境问题"><a href="#解决师兄指定模型跑时的环境问题" class="headerlink" title="解决师兄指定模型跑时的环境问题"></a>解决师兄指定模型跑时的环境问题</h3><pre><code class="python">import llmcompressor==0.4.1
# 将原示例代码from llmcompressor import oneshot 替换为
# from llmcompressor.transformers.finetune import oneshot
</code></pre>
<hr>
<h1 id="2025-4-20"><a href="#2025-4-20" class="headerlink" title="2025&#x2F;4&#x2F;20"></a>2025&#x2F;4&#x2F;20</h1><h3 id="文件传输经验"><a href="#文件传输经验" class="headerlink" title="文件传输经验"></a>文件传输经验</h3><blockquote>
<p>本机Windows，从本地传输大量文件到云服务器的经验</p>
<p><em>LordShark</em>一早上快被文件传输整疯了，希望自己的经验对uu们有帮助，不用再浪费时间在查找文件传输办法上了&#x2F;(ㄒoㄒ)&#x2F;</p>
</blockquote>
<p>打开PowerShell，输入<code>scp -r -P 1234 &quot;C:\...\...\&quot; username@cloud_ip:/.../</code> <br><br>-r 代表递归复制文件和文件夹 <br><br>-P 1234代表指定端口1234【如果租的机子名称一样，则一定要指定端口号，否则连接失败】<br><br>“C:...&quot;代表本地要传输的文件夹位置 <br><br>username 云服务器用户名（一般为root）<br><br>cloud_ip 云服务器ip（可以在ssh登录处获取）<br><br>后面是云服务器上要存放的位置（如果文件夹不存在，需要手动创建文件夹）<br><br>如果是第一次连接会向你确认，输入”yes”即可<br><br>然后输入密码，就是ssh连接时的密码（密码隐形，屏幕上不可见，输入后直接回车即可）<br><br>就可以愉快地传输数据了[]~ (￣▽￣) ~* <br></p>
<hr>
<h1 id="2025-4-21"><a href="#2025-4-21" class="headerlink" title="2025&#x2F;4&#x2F;21"></a>2025&#x2F;4&#x2F;21</h1><h3 id="conda管理Python虚拟环境"><a href="#conda管理Python虚拟环境" class="headerlink" title="conda管理Python虚拟环境"></a>conda管理Python虚拟环境</h3><blockquote>
<p>在开发不同的项目时，不同的项目可能依赖于不同版本的库。通过创建虚拟环境，可以为每个项目创建一个独立的环境</p>
<p>在虚拟环境中安装&#x2F;删除包 不会影响全局环境(可以随便乱造bushi)</p>
</blockquote>
<p>打开终端，输入<code>conda create -n myenv</code> 可以创建一个名为myenv的虚拟环境(体现为一个文件夹)<br><br>想要指定具体python版本可以在后面跟python&#x3D;3.8<br><br>Linux激活虚拟环境 输入<code>source activate myenv</code><br><br>Windows激活虚拟环境 输入<code>conda activate myenv</code><br><br>看见终端前面会有<code>env</code>出现，代表已经进入虚拟环境 <br><br>然后就可以随便造 : ) <br><br>关闭环境<code>conda deactivate</code><br><br>查看已创建的环境<code>conda env list</code> <br><br>删除已创建的环境<code>conda remove -n myenv --all</code><br><br>查询环境中有哪些包<code>conda list</code><br></p>
<hr>
<h1 id="2025-4-22"><a href="#2025-4-22" class="headerlink" title="2025&#x2F;4&#x2F;22"></a>2025&#x2F;4&#x2F;22</h1><h3 id="screen实现后台运行"><a href="#screen实现后台运行" class="headerlink" title="screen实现后台运行"></a>screen实现后台运行</h3><blockquote>
<p>为了解决跑代码的时候无法关闭页面狂玩的问题(bushi) <em>LordShark</em>查了一下如何在后台运行程序<br>大模型的训练和评测可能花费大量时间 而且as we all know 屏幕黑了或者关闭Jupyter都会触发终端自动挂断</p>
<p>那么如何在服务器开着、电脑关闭的情况下运行代码呢？（可以乘此机会狂玩手机bushi）</p>
</blockquote>
<p>打开终端，输入<code>conda list</code>检查一下有没有下载screen<br><br>没有下载就输入<code>pip install screen</code><br><br>输入<code>screen</code>看到弹出页面&#x2F;终端页面更换，就是已经打开screen，按下空格或回车跳过阅读文档<br><br>已经打开screen工具，程序在里面运行就是在后台运行的<br><br>要回到原终端的话，按下<code>Ctrl+A+D</code><br><br>要回到程序：输入<code>screen -r</code><br><br>结束后台程序：screen页面输入<code>exit</code><br></p>
<h1 id="2025-4-21-1"><a href="#2025-4-21-1" class="headerlink" title="2025&#x2F;4&#x2F;21"></a>2025&#x2F;4&#x2F;21</h1><h3 id="github上面的实例代码完整跑通的流程"><a href="#github上面的实例代码完整跑通的流程" class="headerlink" title="github上面的实例代码完整跑通的流程"></a>github上面的实例代码完整跑通的流程</h3><p><em>环境和包</em></p>
<pre><code class="bash">!pip install llmcompressor
!pip install vllm
!pip install huggingface_hub
</code></pre>
<p><em>实际运行代码</em></p>
<blockquote>
<p>从镜像网站上下载模型到本地，解决直接跑代码时远程获取模型链接超时的问题。</p>
</blockquote>
<pre><code class="python">import os
import subprocess

# 设置镜像站点
os.environ[&#39;HF_ENDPOINT&#39;] = &#39;https://hf-mirror.com&#39;

try:
    subprocess.run([
        &quot;huggingface-cli&quot;, 
        &quot;download&quot;, 
        &quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;, 
        &quot;--local-dir&quot;, 
        &quot;./models&quot;
    ])
except Exception as e:
    print(f&quot;下载模型时出错: {e}&quot;)
</code></pre>
<blockquote>
<p>运用本地的模型进行量化</p>
</blockquote>
<pre><code class="python">from llmcompressor.modifiers.smoothquant import SmoothQuantModifier
from llmcompressor.modifiers.quantization import GPTQModifier
from llmcompressor import oneshot


# 选择量化算法
recipe = [
    SmoothQuantModifier(smoothing_strength=0.8),
    GPTQModifier(scheme=&quot;W8A8&quot;, targets=&quot;Linear&quot;, ignore=[&quot;lm_head&quot;]),
]

# 应用量化
oneshot(
    model=&quot;models&quot;,
    dataset=&quot;open_platypus&quot;,
    recipe=recipe,
    output_dir=&quot;TinyLlama-1.1B-Chat-v1.0-INT8&quot;,
    max_seq_length=2048,
    num_calibration_samples=512,
)
</code></pre>
<h1 id="秋-2025-4-22"><a href="#秋-2025-4-22" class="headerlink" title="秋 2025&#x2F;4&#x2F;22"></a>秋 2025&#x2F;4&#x2F;22</h1><h2 id="评估库的使用参考知乎："><a href="#评估库的使用参考知乎：" class="headerlink" title="评估库的使用参考知乎："></a>评估库的使用参考知乎：</h2><blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/671235487">https://zhuanlan.zhihu.com/p/671235487</a></p>
</blockquote>
<h3 id="最主要的问题是评估使用的部分数据集无法被直接下载"><a href="#最主要的问题是评估使用的部分数据集无法被直接下载" class="headerlink" title="最主要的问题是评估使用的部分数据集无法被直接下载"></a>最主要的问题是评估使用的部分数据集无法被直接下载</h3><h2 id="或者在用终端运行评估时，如果出现一整屏的warning，那么很有可能是远程的数据集链接不上了，可以输入一下设置镜像的指令"><a href="#或者在用终端运行评估时，如果出现一整屏的warning，那么很有可能是远程的数据集链接不上了，可以输入一下设置镜像的指令" class="headerlink" title="或者在用终端运行评估时，如果出现一整屏的warning，那么很有可能是远程的数据集链接不上了，可以输入一下设置镜像的指令"></a>或者在用终端运行评估时，如果出现一整屏的warning，那么很有可能是远程的数据集链接不上了，可以输入一下设置镜像的指令</h2><p>可以使用</p>
<pre><code class="bash">export HF_ENDPOINT=https://hf-mirror.com
</code></pre>
<p>设置镜像后，在终端运行python代码写的量化程序</p>
<h1 id="秋-2025-4-24"><a href="#秋-2025-4-24" class="headerlink" title="秋 2025&#x2F;4&#x2F;24"></a>秋 2025&#x2F;4&#x2F;24</h1><h1 id="2025-4-25"><a href="#2025-4-25" class="headerlink" title="2025&#x2F;4&#x2F;25"></a>2025&#x2F;4&#x2F;25</h1><h2 id="nohup使用方法"><a href="#nohup使用方法" class="headerlink" title="nohup使用方法"></a>nohup使用方法</h2><p>类似screen 但是screen是切换终端页面 nohup在后台运行结束后生成output.log日志<br>nohup运行python代码：</p>
<pre><code>nohup python test.py &gt; output.log 2&gt;&amp;1 &amp;
</code></pre>
<p>查看output.log：<br><code>tail -f output.log</code><br><code>less output.log</code></p>
